#-----------------Tilt Matching Training Config-----------------
tm:
  a_end: 100.0
  buffer_chunk_size: 16
  buffer_refresh_steps: 16
  control_variate: 1.0
  grad_accum_steps: 4
  h: 1.0
  loss_type: itm
  num_batch_prompts: 1
  num_buffer_prompts: 16
  num_buffer_refresh: 4
  num_completions_per_prompt: 4
  steps_per_h: 512 # this is the number of optimizer steps per h

#-----------------Optimizer Config-----------------
adam_beta1: 0.9
adam_beta2: 0.99
adam_epsilon: 1e-8
weight_decay: 0.1

max_grad_norm: 0.8
learning_rate: 3e-6
lr_decay_ratio: 0.25
lr_min: 1e-6
lr_scheduler_type: linear
lr_warmup_ratio: 0.1

#-----------------Diffusion Generation Config-----------------
block_length: 32
cfg_scale: 0.0 #TODO: check
diffusion_steps: 128
max_completion_length: 256
max_prompt_length: 1132  # for sudoku with few_shot=3
remasking_strategy: low_confidence # or random
sampling_temperature: 1.0

#-----------------Model Config-----------------
use_peft: true
torch_dtype: bfloat16
load_in_4bit: true
base_model_path: /n/netscratch/albergo_lab/Everyone/frank/hf_models/LLaDA-8B-Instruct
# don't implement any attention for llada

lora_r: 128
lora_alpha: 64
lora_dropout: 0.05
peft_task_type: CAUSAL_LM

#-----------------Logging and Checkpointing Config-----------------
checkpoint_dir: /n/netscratch/albergo_lab/Everyone/frank/llada_tm
checkpoint_freq: 2.0  # in units of h
metrics_log_every: 10
resume_path:
# leave empty if not resuming
wandb:
  entity: "yuyuanchen-harvard-university"
  project: "LLaDA_TM"
  name: "test_sudoku"

#-----------------Other Training Config-----------------
dataset: sudoku_new
few_shot: 3
seed: 42
nodes: 2
devices: 4  # 8? 4?

#-----------------Tilt Matching Training Config-----------------
tm:
  a_end: 1.0
  buffer_chunk_size: 16
  buffer_refresh_steps: 16
  control_variate: 1.0
  h: 0.05
  loss_type: itm
  num_batch_prompts: 1
  num_buffer_prompts: 2
  num_buffer_refresh: 1
  num_completions_per_prompt: 2
  steps_per_h: 16 # for phase transition test only

#-----------------Optimizer Config-----------------
adam_beta1: 0.9
adam_beta2: 0.99
adam_epsilon: 1e-8
weight_decay: 0.1

max_grad_norm: 0.2
learning_rate: 3e-6
lr_decay_ratio: 0.2
lr_min: 0.0
lr_scheduler_type: linear
lr_warmup_ratio: 0.1

#-----------------Diffusion Generation Config-----------------
block_length: 32
cfg_scale: 0.0 #TODO: check
diffusion_steps: 128
max_completion_length: 256
max_prompt_length: 1132  # for sudoku with few_shot=3
remasking_strategy: low_confidence # or random
sampling_temperature: 1.0

#-----------------Model Config-----------------
use_peft: true
torch_dtype: bfloat16
load_in_4bit: true
base_model_path: /n/netscratch/albergo_lab/Everyone/frank/hf_models/LLaDA-8B-Instruct
attn_implementation: flash_attention_2 # TODO: What's this?

lora_r: 128
lora_alpha: 64
lora_dropout: 0.05
peft_task_type: CAUSAL_LM

#-----------------Logging and Checkpointing Config-----------------
checkpoint_dir: /n/netscratch/albergo_lab/Everyone/frank/llada_tm
checkpoint_freq: 0.2  # in units of h
metrics_log_every: 10
resume_path:  # leave empty if not resuming
wandb:
  entity: "yuyuanchen-harvard-university"
  project: "LLaDA_TM"
  name: "test_skeleton"

#-----------------Other Training Config-----------------
dataset: sudoku_new
few_shot: 3
seed: 42
nodes: 1
devices: 1 # 8? 4?